<!doctype html>
<html lang="en">
<head>
    <title>Probability for Engineers - Leo Qi</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="ECE 203 course notes" />
    <meta charset="UTF-8" />
    <link rel="stylesheet" href="/assets/latex.vercel.app.style.min.css" />
    <script id="MathJax-script" async src="/assets/npm.mathjax3.es5.tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="/assets/latex.vercel.app.prism.css" />
    <script async defer src="/assets/npm.prism.min.js"></script>
    <link rel="icon" href="/favicon.ico" />
</head>
<body class="libertinus">
  <header>
    <h1>Probability for Engineers</h1>
    <p class="author">Leo Qi <br> December 3rd, 2024</p>
  </header>
  <div class="abstract">
    <h2>Abstract</h2>
    <p>Let's document our discovery of the ideas of probability.</p>
  </div>
  <nav role="navigation" class="toc">
    <h2>Contents</h2>
    <ol>
      <li><a href="#probability-axioms">The axioms of probability theory</a></li>
      <li><a href="#random-variables">Random variables</a></li>
      <ol>
        <li><a href="#continuous-rv">Continuous random variables</a></li>
      </ol>
      <li><a href="#jointly-distributed-rv">Jointly distributed random variables</a></li>
    </ol>
  </nav>
  <main>
  <article class="indent-pars">
    <h2 id="probability-axioms">The axioms of probability theory</h2>
    <div class="definition">
      A probability space is a triplet \((\Omega, \mathcal{F}, P)\).
      \(\Omega\) is a nonempty set called the <em>sample space</em>,
      \(\mathcal{F}\) is a set of subsets of \(\Omega\) called <em>events</em>,
      and \(P\) is called a <em>probability measure</em> on \(\mathcal{F}\).
      <span class="sidenote">
        A probability measure is a measure, which is a function satisfying
        certain properties. We define additional axioms it must satisfy.
      </span>
    </div>
    <div class="definition">
      \(\Omega\) is a nonempty set called the <em>sample space</em>. Each
      element \(\omega \in \Omega\) is called an <em>outcome</em>.
    </div>
    <div class="definition">
      Events \(A_i\in\mathcal{F}\) are mutually exclusive, also known as disjoint,
      if \(A_i\cap A_j = \emptyset\) for all \(i\ne j\).
    </div>
    <p>
      We can now define the function that assigns probabilities to events
      in the sample space. This function must satisfy the axioms of probability
      theory.
    </p>
    <div class="definition">
      A probability measure is any measure \(P\) on \(\mathcal{F}\)
      that satisfies the following axioms:

      <ol>
        <li>\(P(A) \ge 0\, \forall A\in\mathcal{F}\)</li>
        <li>\(P(\Omega) = 1\)</li>
        <li>If \(A_1, A_2, \ldots\) are disjoint events, then
          \(P(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)\)</li>
      </ol>
    </div>
    <p>
      The probability measure assigns a real number to each event that
      represents how likely it is to happen. This is called the <em>probability</em>
      of the event. For the math to make sense, this number must be nonnegative.
      The probability of the entire sample space is 1, so any event that has
      probability 1 is certain to happen, and there cannot be a higher probability.
      We can also derive more properties from these axioms. For any subsets \(A, B, C\in\mathcal{F}\):
      <span class="sidenote">
        \(P\) did not have to be defined this way. The only
        requirement is that it is a &#8220;positive continuous monotonic function.&#8221;
        An alternate definition could have been from infinity (impossible)
        <em>down</em> to one (certain)! To do this, take the reciprocal of 
        any probability as defined here. <sup><a href="#fn1" id="ref1">1</a></sup>
      </span>
    </p>
    <ol>
      <li>\(A\subseteq B \implies P(A) \le P(B)\)</li>
      <li>\(P(A^c) = 1 - P(A)\)</li>
      <li>\(P(A\cup B) = P(A) + P(B) - P(A\cap B)\)</li>
      <li>\(P(\emptyset) = 0\)</li>
    </ol>

    <h2 id="random-variables">Random variables</h2>
    <div class="definition">
      A random variable (r.v.) is any function \(X:\Omega\to\mathbb{R}\) that is
      \(\mathcal{F}\) measurable.<sup><a href="#fn2" id="ref2">2</a></sup>

      <span class="sidenote">
        This means that for any number \(c\),
        \(\{\omega\in\Omega: X(\omega) \le c\}\in\mathcal{F}\). I don't understand
        this yet.
      </span>
    </div>
    <p>
      If the sample space is finite or countably infinite, \(\mathcal{F}\) is
      most often defined to be the power set of \(\Omega\). Then any function from
      \(\Omega\) to \(\mathbb{R}\) will be \(\mathcal{F}\) measurable and thus a
      random variable.
      Therefore, for practical purposes we may assume that any function that
      maps outcomes (which have corresponding probabilities) to real numbers is a
      random variable.
    <p>
    <p>
      We use random variables when we are interested in some function of the
      outcome as opposed to the actual outcome itself. <a href="#fn6" id="ref6">[6]</a>
      For example, let us model the experiment of flipping a coin five times.
      The probability space would be \((\Omega, \mathcal{F}, P)\), and
      \(\Omega = \{H, T\}^5\). To find the probability that a certain number of
      heads appears, we can then define a r.v. \(Z\) with the mapping:
      \[ Z: \Omega\to\mathbb{R} = \left\{ \begin{matrix} HHHHH \mapsto 5 \\
        HHHHT\mapsto 4 \\
        HHHTH\mapsto 4 \\
        HHTHH\mapsto 4 \\
        HTHHH\mapsto 4 \\
        THHHH \mapsto 4 \\ 
        \ldots \\
        TTTTT \mapsto 0
        \end{matrix}\right . \]
      The probability that there are 4 heads can then be written as \(P[Z=4]\).
    </p>
    <p>
      <span class="sidenote">
        The distinction between discrete and continuous r.v. is a technical
        one, owing to the fact that if the sample space is uncountable,
        how do we describe the probability of a continuous r.v. taking on an exact value?
        We may only ask for the probability that it is <em>close to</em> an
        exact value. The pdf is a way to describe this, and our sums become
        integrals. <a href="#fn7" id="ref7">[7]</a>
      </span>
      Random variables can be discrete or continuous. Discrete random variables
      model experiments with a finite or countably infinite number of outcomes,
      while continuous random variables model experiments that deal with
      continuous quantities like time or distance.
    </p>
    <div class="definition">
      The probability mass function (pmf) of a discrete random variable \(X\) is
      the function \(p_X(x) = P[X=x]\). It is the mapping

      \[\forall x\in\text{Range}(X), x \mapsto \sum_{\{\omega\in\Omega : X(\omega) = x\}} P[{\omega}]\]
    </div>
    <div class="definition">
      The cumulative distribution function (cdf) of a random variable \(X\) is
      the function \(F_X(x) = P[X\le x]\). Loosely rendered, it is the mapping

      \[\forall x\in\text{Range}(X), x \mapsto \sum_{\{\omega\in\Omega : X(\omega) \le x\}} P[{\omega}]\]

      The cdf is a right-continuous monotone increasing function (known as cadlag).
    </div>
    <div class="definition">
      The expected value of a discrete random variable \(X\) is:

      \[E[X] = \sum_{x\in\text{Range}(X)} x p_X(x)\]

      This is a weighted average of the range of \(X\). The weight is the
      probability of \(X\) taking each value.
    </div>

    <h3 id="continuous-rv">Continuous random variables</h3>
    <div class="definition">
      \(X\) is a <em>continuous random variable</em> if there exists a non-negative
      function \(f_X(x)\) such that for any event \(B\in\mathcal{F}\):

      \[P[X\in B] = \int_B f_X(x)dx\]

      called the probability density function (pdf) of \(X\).
    </div>
    <p>
      We integrate over the space of an event \(B\). Recall
      that \(X: \Omega \to \mathbb{R}\), so essentially the pdf maps each value
      of the range of \(X\) to its probability.


      all the outcomes in the sample space that map to
      that value 
      its probability.

      Evaluated in an integral, the pdf represents the likelihood
      that \(X\) takes on a value in that range. This gives us a straightforward
      way to define the expected value of continuous r.v:
    </p>
    <div class="definition">
      The expected value of a continuous random variable \(X\) is:

      \[E[X] = \int_{-\infty}^\infty xf_X(x)dx\]
    </div>
    <p>
      Since \(X\) must take a value in \(\mathbb{R}\), this
      gives us a weighted average. The weight is the probability of \(X\) taking
      a particular value, and the average is of the values themselves.
    </p>
    <p>
      What happens when we take the value of a random variable, and then apply
      another function to it? It is hard sometimes to define its pdf (if it maps
      multiple values to the same value in some way) but we can always define its
      expected value as a new weighted average.

      \[E[g(X)] = \int_{-\infty}^\infty g(x)f_X(x)dx\]
    </p>


    <h2>Bayes' theorem</h2>
    <ul>
      <li>Tests are not events. We have a cancer test, separate from the event
      of actually having cancer.</li>
      <li>Tests are flawed. They may detect things that don't exist (false 
      positives) or fail to detect things that do exist (false negatives).</li>
      <li>False positives skew results. Suppose the test is 99% accurate. If
      the disease is rare, the test will be wrong more often than right.</li>
      <li>People prefer natural numbers. Saying &#8220;1 in 1000&#8221; is more
      understandable than &#8220;0.1%&#8221;.</li>
      <li>Even science is a test. Scientific experients are potentially flawed
        and need to be treated accordingly. There is a test for a phenomenon and
        then there is the event of the phenomenon itself.</li>
    </ul>
    <p>
      Bayes' theorem converts the results from your test into the real probability
      of the event.<sup><a href="#fn3" id="ref3">3</a></sup> We can correct for
      measurement errors if you know the real
      probabilities and the chance of a false positive and false negative, and
      relate the actual probability to the measured test probability.
    </p>
    <p>
      This is finding \(P[H|E]\), the chance that a hypothesis is true given
      evidence \(E\), starting from \(P[E|H]\), the chance that the evidence
      appears when the hypothesis is true.

      \[P[H|E] = \frac{P[E|H]P[H]}{P[E|H]P[H] + P[E|H^c]P[H^c]}\]
    </p>

    <h2 id="conditional-expectation">Conditional expectation</h2>
    <p>
      For two discrete r.v. \(X\) and \(Y\), we may define the conditional
      pmf of \(X\) given \(Y\) as:

      \[p_{X|Y}(x|y) = P[X=x|Y=y] = {p_{XY}(x, y)\over p_Y(y)}\]

      given that \(p_Y(y) > 0\).
    </p>
    <div class="definition">
      The conditional expectation of a random variable \(X\) given an event \(A\) is:

      \[E[X|Y = y] = \sum_x x p_{X|Y}(x|y)\]
    </div>
    <p>
      For continuous r.v., the conditional expectation is defined using the
      conditional pdf:

      \[E[X|Y = y] = \int_{-\infty}^\infty x f_{X|Y}(x|y)dx\]

      In both cases, this expectation satisfies the properties of an ordinary
      expectation. Most importantly,

      \[E\left[ \sum^n_{i=1} X_i | Y = y\right] = \sum^n_{i=1} E[X_i | Y = y]\]

      Further, the law of total expectation (also known as the law of
      iterated expectations) states that

      \[E[X] = E[E[X|Y]]\]
    </p>
    <p>
      Conditioning can be used to compute probabilities. Let \(A\) be an event,
      \(Y\) be a random variable such that

      \[Y\in\{y_1, y_2, \ldots \}\]

      and \(B_i\) be the event that \(Y = y_i\). Then by the law of total
      probability,

      \[P[A] = \sum_i P[A|B_i]P[B_i]\]

      since \(B\) is a partition of the sample space.

      In continuous form, this looks like

      \[P[A] = \int_{-\infty}^\infty P[A|Y=y]f_Y(y)dy\]
    </p>

    <h2 id="moment-generating-functions">Moment generating functions</h2>
    <div class="definition">
      The moment generating function (mgf) of a random variable \(X\) is the
      function \(M_X(t) = E[e^{tX}]\).

      \[ M_X(t) = \left\{ \begin{matrix} \sum_x e^{tx} p_X(x) & \text{discrete} \\
        \int_{-\infty}^\infty e^{tx} f_X(x)dx & \text{continuous} \end{matrix}\right . \]
    </div>
    <p>
      The mgf is useful because it allows us to calculate the nth moment of a
      random variable.

      \[E[X^n] = \left. \frac{d^n}{dt^n} M_X(t) \right|_{t=0}\]
    </p>

    <h2 id="limit-theorems">Limit theorems</h2>
    <div class="lemma">
      Markov inequality: If \(X\) is a non-negative random variable and \(a>0\):

      \[P[X\ge a] \le \frac{E[X]}{a}\]
    </div>

    <div class="theorem">
      Chebyshev's inequality: If \(X\) is a random variable with mean \(\mu\)
      and variance \(\sigma^2\), then for any \(b>0\):

      \[P[|X-\mu|\ge b] = P[(X-\mu)^2\ge b^2] \le \frac{\sigma^2}{b^2}\]
    </div>
    <p>
      This result provides an upper bound for the probability that a random
      variable deviates from its mean.
    </p>

    <div class="theorem">
      Weak law of large numbers: Let \(X_1, X_2, \ldots\) be independent and
      identically distributed random variables (iid) with common mean \(\mu\).
      Then, for any \(\epsilon\ge 0\):

      \[P\left[\left|\frac{1}{n}\sum_{i=1}^n X_i - \mu\right| \ge \epsilon\right] \to 0\]

      as \(n\to\infty\).
    </div>

    <div class="theorem">
      The central limit theorem: Let \(X_1, X_2, \ldots\) be independent and
      identically distributed random variables (iid) with common mean \(\mu\)
      and variance \(\sigma^2\). Then, the distribution of

      \[Z_n = \frac{1}{\sqrt{n}}\sum_{i=1}^n \frac{X_i - \mu}{\sigma}\]

      converges to the standard normal distribution as \(n\to\infty\).
    </div>

    <div class="theorem">
      The strong law of large numbers: Let \(X_1, X_2, \ldots\) be independent
      and identically distributed random variables (iid) with common mean
      \(E[X_i] = \mu\). Then

      \[ P\left[\lim_{n\to\infty} \frac{1}{n}\sum_{i=1}^n X_i = \mu\right] = 1\]
    </div>

    <h2 id="jointly-distributed-rv">Jointly distributed random variables</h2>
    <p>
      In many applications, we are interested in mulitple dependent random
      variables and the strength of their dependence. We can start by defining
      the joint cumulative distribution function (cdf) of two random variables
      \(X\) and \(Y\) in the same probability space, since it exists for
      both discrete and continuous random variables.
    </p>
    <div class="definition">
      The joint <em>cumulative distributon function</em> (cdf) of \(X\) and
      \(Y\) is the function of two variables defined by

      \[ F_{X, Y}(u, v) = P\{X\le u, Y\le v\} \forall (u, v)\in\mathbb{R}^2\]
    </div>
    <p>
      To reiterate, this is a mapping from a plane of points \(X, Y\) to one
      probability value representing the likelihood that the outcome will be
      mapped to the region \(\{(x, y): x\le u, y\le v\}\). To find the probability
      that a point is mapped within any rectangle \(R = (a, b]\times(c, d]\),
      the equation is

      \[P\{(X, Y)\in R\} = F_{X, Y}(b, d) - F_{X, Y}(a, d) - F_{X, Y}(b, c) + F_{X, Y}(a, c)\]

      Why this is so can be understood by drawing the overlapping regions of
      \(\{(x, y): x\le b, y\le d\}\) and \(\{(x, y): x\le a, y\le c\}\).
    </p>
    <figure>
      <center>
        <img src="/images/joint-cdf.png" alt="Joint CDF" loading="lazy" width="400" height="300" />
      </center>
      <figcaption>Probability of \(R\)</figcaption>
    </figure>
    <div class="definition">
      The marginal cdf of a random variable \(X\) is the cdf of \(X\) alone,
      given its joint cdf with another random variable \(Y\):

      \[F_X(x) = \lim_{y\to\infty} F_{X, Y}(x, y)\]
    </div>
    <p>
      With the joint cdf, we can define the joint probability mass function (pmf)
      for the discrete case and the joint probability density function (pdf) for
      the continuous case.
    </p>
    <div class="definition">
      The joint <em>probability mass function</em> (pmf) of two discrete random
      variables \(X\) and \(Y\) is the function

      \[p_{X, Y}(x, y) = P[X=x, Y=y]\]
    </div>

    <h3 id="joint-continuous-rv">Joint continuous random variables</h3>
    <div class="definition">
      The random variables \(X\) and \(Y\) are jointly continuous if there exists
      a non-negative function \(f_{X, Y}(x, y)\) (the joint pdf) such that

      \[F_{X, Y}(x, y) = \int_{-\infty}^x\int_{-\infty}^y f_{X, Y}(u, v)dvdu\]
    </div>
    <p>
      By definition, the joint pdf is the derivative of the joint cdf:

      \[f_{X, Y}(x, y) = \frac{\partial^2}{\partial x\partial y} F_{X, Y}(x, y)\]

      And the probability of the random variables falling within any region \(R\)
      that has a piecewise differentiable boundary is:

      \[P\{(X, Y)\in R\} = \int\int_R f_{X, Y}(x, y)dxdy\]
    </p>
    <div class="definition">
      The expectation of two jointly continuous random variables \(X\) and \(Y\)
      is:

      \[E[g(X, Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x, y)f_{X, Y}(x, y)dxdy\]
    </div>
    <p>
      This implies that expectation is still linear for jointly continuous
      r.v. For example, \(E[aX + bY + c] = aE[X] + bE[Y] + c\).
    </p>

    <div class="footnotes">
      <p class="no-indent">Last updated: 2024-12-07 9:18 EST</p>
      <p class="no-indent" id="fn1">
        [1] E. T. Jaynes, <em>Probability theory: the logic of science.</em>
        Cambridge University Press, 2003. <a href="#ref1">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn2">
        [2] B. Hajek, <em>Random processes for engineers.</em>
        Cambridge University Press, 2015. <a href="#ref2">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn3">
        [3] K. Azad, &#8220;An intuitive (and short) explanation of bayes' theorem,&#8221;
        Better Explained. <a href="#ref3">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn4">
        [4] K. R. Davidson, &#8220;Measure Theory - Notes for Pure Math 451,&#8221;
        University of Waterloo, 2022. <a href="#ref4">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn5">
        [5] P. Mitran, &#8220;Lecture notes for ECE 203,&#8221;
        University of Waterloo, 2024. <a href="#ref5">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn6">
        [6] S. Ross, <em>A First Course in Probability</em>, Tenth.
        Pearson, 2023. <a href="#ref6">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn7">
        [7] Nykamp DQ, “The idea of a probability density function.”
        From Math Insight. <a href="http://mathinsight.org/probability_density_function_idea">[Online]</a>
        <a href="#ref7">&#x21A9;</a>
    </div>
  </article>
</main>
</body>
</html>

<!doctype html>
<html lang="en">
<head>
    <title>Probability for Engineers - Leo Qi</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="ECE 203 course notes" />
    <meta charset="UTF-8" />
    <link rel="stylesheet" href="/assets/latex.vercel.app.style.min.css" />
    <script id="MathJax-script" async src="/assets/npm.mathjax3.es5.tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="/assets/latex.vercel.app.prism.css" />
    <script async defer src="/assets/npm.prism.min.js"></script>
    <link rel="icon" href="/favicon.ico" />
</head>
<body class="libertinus">
  <header>
    <h1>Probability for Engineers</h1>
    <p class="author">Leo Qi <br> December 3rd, 2024</p>
  </header>
  <div class="abstract">
    <h2>Abstract</h2>
    <p>Let's document our discovery of the ideas of probability.</p>
  </div>
  <nav role="navigation" class="toc">
    <h2>Contents</h2>
    <ol>
      <li><a href="#probability-axioms">The axioms of probability theory</a></li>
      <li><a href="#random-variables">Random variables</a></li>
    </ol>
  </nav>
  <main>
  <article class="indent-pars">
    <h2 id="probability-axioms">The axioms of probability theory</h2>
    <div class="definition">
      A probability space is a triplet \((\Omega, \mathcal{F}, P)\).
      \(\Omega\) is a nonempty set called the <em>sample space</em>,
      \(\mathcal{F}\) is a set of subsets of \(\Omega\) called <em>events</em>,
      and \(P\) is called a <em>probability measure</em> on \(\mathcal{F}\).
      <span class="sidenote">
        A probability measure is a measure, which is a function satisfying
        certain properties. We define additional axioms it must satisfy.
      </span>
    </div>
    <div class="definition">
      \(\Omega\) is a nonempty set called the <em>sample space</em>. Each
      element \(\omega \in \Omega\) is called an <em>outcome</em>.
    </div>
    <div class="definition">
      Events \(A_i\in\mathcal{F}\) are mutually exclusive, also known as disjoint,
      if \(A_i\cap A_j = \emptyset\) for all \(i\ne j\).
    </div>
    <p>
      We can now define the function that assigns probabilities to events
      in the sample space. This function must satisfy the axioms of probability
      theory.
    </p>
    <div class="definition">
      A probability measure is any measure \(P\) on \(\mathcal{F}\)
      that satisfies the following axioms:

      <ol>
        <li>\(P(A) \ge 0\, \forall A\in\mathcal{F}\)</li>
        <li>\(P(\Omega) = 1\)</li>
        <li>If \(A_1, A_2, \ldots\) are disjoint events, then
          \(P(\bigcup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)\)</li>
      </ol>
    </div>
    <p>
      The probability measure assigns a real number to each event that
      represents how likely it is to happen. This is called the <em>probability</em>
      of the event. For the math to make sense, this number must be nonnegative.
      The probability of the entire sample space is 1, so any event that has
      probability 1 is certain to happen, and there cannot be a higher probability.
      We can also derive more properties from these axioms. For any subsets \(A, B, C\in\mathcal{F}\):
      <span class="sidenote">
        \(P\) did not have to be defined this way. The only
        requirement is that it is a &#8220;positive continuous monotonic function.&#8221;
        An alternate definition could have been from infinity (impossible)
        <em>down</em> to one (certain)! To do this, take the reciprocal of 
        any probability as defined here. <sup><a href="#fn1" id="ref1">1</a></sup>
      </span>
    </p>
    <ol>
      <li>\(A\subseteq B \implies P(A) \le P(B)\)</li>
      <li>\(P(A^c) = 1 - P(A)\)</li>
      <li>\(P(A\cup B) = P(A) + P(B) - P(A\cap B)\)</li>
      <li>\(P(\emptyset) = 0\)</li>
    </ol>

    <h2 id="random-variables">Random variables</h2>
    <div class="definition">
      A random variable (r.v.) is any function \(X:\Omega\to\mathbb{R}\) that is
      \(\mathcal{F}\) measurable.<sup><a href="#fn2" id="ref2">2</a></sup>

      <span class="sidenote">
        This means that for any number \(c\),
        \(\{\omega\in\Omega: X(\omega) \le c\}\in\mathcal{F}\). I don't understand
        this yet.
      </span>
    </div>
    <p>
      If the sample space is finite or countably infinite, \(\mathcal{F}\) is
      most often defined to be the power set of \(\Omega\). Then any function from
      \(\Omega\) to \(\mathbb{R}\) will be \(\mathcal{F}\) measurable and thus a
      random variable.
      Therefore, for practical purposes we may assume that any function that
      maps outcomes (which have corresponding probabilities) to real numbers is a
      random variable.
    <p>
    <p>
      We use random variables when we are interested in some function of the
      outcome as opposed to the actual outcome itself. <a href="#fn6" id="ref6">[6]</a>
      For example, let us model the experiment of flipping a coin five times.
      The probability space would be \((\Omega, \mathcal{F}, P)\), and
      \(\Omega = \{H, T\}^5\). To find the probability that a certain number of
      heads appears, we can then define a r.v. \(Z\) with the mapping:
      \[ Z: \Omega\to\mathbb{R} = \left\{ \begin{matrix} HHHHH \mapsto 5 \\
        HHHHT\mapsto 4 \\
        HHHTH\mapsto 4 \\
        HHTHH\mapsto 4 \\
        HTHHH\mapsto 4 \\
        THHHH \mapsto 4 \\ 
        \ldots \\
        TTTTT \mapsto 0
        \end{matrix}\right . \]
      The probability that there are 4 heads can then be written as \(P[Z=4]\).
    </p>
    <div class="definition">
      The probability mass function (pmf) of a discrete random variable \(X\) is
      the function \(p_X(x) = P[X=x]\). It is the mapping

      \[\forall x\in\text{Range}(X), x \mapsto \sum_{\{\omega\in\Omega : X(\omega) = x\}} P[{\omega}]\]
    </div>

    <h3 id="continuous-rv">Continuous random variables</h3>
    <div class="definition">
      \(X\) is a <em>continuous random variable</em> if there exists a non-negative
      function \(f_X(x)\) such that for any event \(B\in\mathcal{F}\):

      \[P[X\in B] = \int_B f_X(x)dx\]

      called the probability density function (pdf) of \(X\).
    </div>
    <p>
      We integrate over the space of an event \(B\). Recall
      that \(X: \Omega \to \mathbb{R}\), so essentially the pdf maps each value
      of the range of \(X\) to its probability.


      all the outcomes in the sample space that map to
      that value 
      its probability.

      Evaluated in an integral, the pdf represents the likelihood
      that \(X\) takes on a value in that range. This gives us a straightforward
      way to define the expected value of continuous r.v:
    </p>
    <div class="definition">
      The expected value of a continuous random variable \(X\) is:

      \[E[X] = \int_{-\infty}^\infty xf_X(x)dx\]
    </div>
    <p>
      Since \(X\) must take a value in \(\mathbb{R}\), this
      gives us a weighted average. The weight is the probability of \(X\) taking
      a particular value, and the average is of the values themselves.
    </p>
    <p>
      What happens when we take the value of a random variable, and then apply
      another function to it? It is hard sometimes to define its pdf (if it maps
      multiple values to the same value in some way) but we can always define its
      expected value as a new weighted average.

      \[E[g(X)] = \int_{-\infty}^\infty g(x)f_X(x)dx\]
    </p>


    <h2>Bayes' theorem</h2>
    <ul>
      <li>Tests are not events. We have a cancer test, separate from the event
      of actually having cancer.</li>
      <li>Tests are flawed. They may detect things that don't exist (false 
      positives) or fail to detect things that do exist (false negatives).</li>
      <li>False positives skew results. Suppose the test is 99% accurate. If
      the disease is rare, the test will be wrong more often than right.</li>
      <li>People prefer natural numbers. Saying &#8220;1 in 1000&#8221; is more
      understandable than &#8220;0.1%&#8221;.</li>
      <li>Even science is a test. Scientific experients are potentially flawed
        and need to be treated accordingly. There is a test for a phenomenon and
        then there is the event of the phenomenon itself.</li>
    </ul>
    <p>
      Bayes' theorem converts the results from your test into the real probability
      of the event.<sup><a href="#fn3" id="ref3">3</a></sup> We can correct for
      measurement errors if you know the real
      probabilities and the chance of a false positive and false negative, and
      relate the actual probability to the measured test probability.
    </p>
    <p>
      This is finding \(P[H|E]\), the chance that a hypothesis is true given
      evidence \(E\), starting from \(P[E|H]\), the chance that the evidence
      appears when the hypothesis is true.

      \[P[H|E] = \frac{P[E|H]P[H]}{P[E|H]P[H] + P[E|H^c]P[H^c]}\]
    </p>

    <div class="footnotes">
      <p class="no-indent">Last updated: 2024-12-03 14:03 EST</p>
      <p class="no-indent" id="fn1">
        [1] E. T. Jaynes, <em>Probability theory: the logic of science.</em>
        Cambridge University Press, 2003. <a href="#ref1">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn2">
        [2] B. Hajek, <em>Random processes for engineers.</em>
        Cambridge University Press, 2015. <a href="#ref2">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn3">
        [3] K. Azad, &#8220;An intuitive (and short) explanation of bayes' theorem,&#8221;
        Better Explained. <a href="#ref3">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn4">
        [4] K. R. Davidson, &#8220;Measure Theory - Notes for Pure Math 451,&#8221;
        University of Waterloo, 2022. <a href="#ref4">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn5">
        [5] P. Mitran, &#8220;Lecture notes for ECE 203,&#8221;
        University of Waterloo, 2024. <a href="#ref5">&#x21A9;</a>
      </p>
      <p class="no-indent" id="fn6">
        [6] S. Ross, <em>A First Course in Probability</em>, Tenth.
        Pearson, 2023. <a href="#ref6">&#x21A9;</a>
    </div>
  </article>
</main>
</body>
</html>
